{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanzanian Water Wells Status Prediction\n",
    "\n",
    "By Melody Bass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!["
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this project is to build a classification model to predict the status of water wells in Tanzania.  The model was built from a dataset of the sources of water and status of the waterpoint using an iterative approach and can be found [here](./data/training_set_values.csv).  The dataset contains information on 54,000 waterpoints in Tanzania.  Following is a description of the features that we will include in our model:\n",
    "* `amount_tsh` - Total static head (amount water available to waterpoint)\n",
    "* `date_recorded` - The date the row was entered\n",
    "* `funder` - Who funded the well\n",
    "* `gps_height` - Altitude of the well\n",
    "* `installer` - Organization that installed the well\n",
    "* `longitude` - GPS coordinate\n",
    "* `latitude` - GPS coordinate\n",
    "* `wpt_name` - Name of the waterpoint if there is one\n",
    "* `num_private` -\n",
    "* `basin` - Geographic water basin\n",
    "* `subvillage` - Geographic location\n",
    "* `region` - Geographic location\n",
    "* `region_code` - Geographic location (coded)\n",
    "* `district_code` - Geographic location (coded)\n",
    "* `lga` - Geographic location\n",
    "* `ward` - Geographic location\n",
    "* `population` - Population around the well\n",
    "* `public_meeting` - True/False\n",
    "* `recorded_by` - Group entering this row of data\n",
    "* `scheme_management` - Who operates the waterpoint\n",
    "* `scheme_name` - Who operates the waterpoint\n",
    "* `permit` - If the waterpoint is permitted\n",
    "* `construction_year` - Year the waterpoint was constructed\n",
    "* `extraction_type` - The kind of extraction the waterpoint uses\n",
    "* `extraction_type_group` - The kind of extraction the waterpoint uses\n",
    "* `extraction_type_class` - The kind of extraction the waterpoint uses\n",
    "* `management` - How the waterpoint is managed\n",
    "* `management_group` - How the waterpoint is managed\n",
    "* `payment` - What the water costs\n",
    "* `payment_type` - What the water costs\n",
    "* `water_quality` - The quality of the water\n",
    "* `quality_group` - The quality of the water\n",
    "* `quantity` - The quantity of water\n",
    "* `quantity_group` - The quantity of water\n",
    "* `source` - The source of the water\n",
    "* `source_type` - The source of the water\n",
    "* `source_class` - The source of the water\n",
    "* `waterpoint_type` - The kind of waterpoint\n",
    "* `waterpoint_type_group` - The kind of waterpoint\n",
    "\n",
    "The first sections focus on investigating, cleaning, wrangling, and engineering some new features.  The next section contains models and evaluation of each, ultimately leading to us to select our best model for predicting waterpoint status.  Finally, I will make recommendations and provide insight on predicting the status of waterpoints making predictions from historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this analysis can be found [here](./data/training_set_values.csv).  It contains a wealth of information about waterpoints in Tanzania and the status of their operation.  The target variable has 3 different options for it's status:\n",
    "* `functional` - the waterpoint is operational and there are no repairs needed\n",
    "* `functional needs repair` - the waterpoint is operational, but needs repairs\n",
    "* `non functional` - the waterpoint is not operational\n",
    "\n",
    "Below I will import the dataset and start my investigation of relevant information it may contain.  Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report,accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Scalers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Categorical Create Dummies\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Import Train Set\n",
    "df_train_set = pd.read_csv('data/training_set_values.csv', index_col='id')\n",
    "df_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import Training set labels\n",
    "df_train_labels = pd.read_csv('data/training_set_labels.csv', index_col='id')\n",
    "df_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "df = pd.merge(df_train_labels, df_train_set, how = 'inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index\n",
    "df.reset_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for categorical data\n",
    "obj_df = df.select_dtypes(include=['object'])\n",
    "obj_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "__scheme_name__ has the most missing values, followed by __funder__, __installer__,  __public_meeting__, __scheme_management__,  and __permit__ with ~3,000 null values, and then __subvillage__ with 371 null values.  Several of these columns will be deleted as they appear to duplicate other columns, and I will investigate __installer__, __permit__, and __subvillage__ further.\n",
    "\n",
    "#### Data types\n",
    "\n",
    "* __wpt_name__, __subvillage__, __ward__, __scheme_name__, __installer__, __funder__, and __date_recorded__ are categorical features that have unique values in the thousands.  This will be a problem with dummy variables, will likely remove. \n",
    "* I will drop __recorded_by__ as it has the same value for all rows.\n",
    "* __num_private__ is not defined on the DrivenData site, and it is not obvious what the feature indicates. \n",
    "* __id__ column will be dropped.\n",
    "* __public_meeting__ and __permit__ are boolean.\n",
    "* __construction_year__, __latitude__, __longitude__, __gps_height, __amount_tsh__, and __population__ all have thousands of rows of 0 entered.  I will drop rows for most of these variables that have 0 entered, and will have to investigate further for real data on some columns.  \n",
    " \n",
    "#### Duplicate  and Similar Data\n",
    " \n",
    "The following columns all contain duplicate or similar data, will remove features that will cause multicollinearity:\n",
    "* __extraction_type__, __extraction_type_group__, and __extraction_type_class__\n",
    "* __payment__ and __payment_type__\n",
    "* __water_quality__ and __quality_group__ \n",
    "* __quanitity__ and __quantity_group__\n",
    "* __source__ and __source_type__\n",
    "* __waterpoint_type__ and __waterpoint_type_group__\n",
    "* __region__ and __region_code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will clean the dataset by removing similar and unnecessary columns and trim the dataset of remaining null values.  I will also further investigate whether some columns contain the same information if it was not immediately obvious.  There are several rows containing 0 enteries in some column information.  I will investigate whether I believe the data to be real instead of a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate and columns with similar information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will keep __extraction_type_class__ and remove __extraction_type__ and __extraction_type_group__ as it's columns values appear to be the most relevant for the project. __scheme_name__ will be dropped for it's many null values.  Other columns will be removed at this point due to irrelavancy, duplicates, null values, and some others will have to be investigated after the first drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns to be dropped\n",
    "dropped_columns = ['extraction_type', 'extraction_type_group', 'payment', 'scheme_name', 'quality_group',\n",
    "                   'quantity_group', 'source', 'waterpoint_type_group', 'recorded_by', 'num_private', 'id', 'subvillage',\n",
    "                  'wpt_name', 'ward', 'funder', 'date_recorded', 'public_meeting', 'region_code', 'district_code', 'lga',\n",
    "                  'scheme_management', 'source_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(dropped_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for nulls\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all remaining null values from our dataset\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check to see that it worked\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert boolean permit to integers\n",
    "df['permit'] = df['permit'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change status_group  to integer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change status_group/target values to numeric values\n",
    "df['status'] = df.status_group.map({\"non functional\":0, \"functional needs repair\":1, \"functional\":2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('status_group', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate management and management_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to investigate these 2 columns further to see if they contain similar information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['management'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['management_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most data is contained in the user-group subcategory of __management_group__.  I will groupby to investigate if the information is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['management_group']=='user-group']['management'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is identical to the data contained in the management column in the subcategory of 'user-group'.  I will drop __management_group__ from our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop column\n",
    "df = df.drop('management_group', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see that it worked\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our first round of cleaning, there are several features we need to examine further:\n",
    "* __status_group__ is an unbalanced target, may need to look into further during modeling.\n",
    "* There are several columns with thousands of 0 entries - __amount_tsh__, __gps_height__, __longitude__, __latitude__, __population__, __construction_year__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['construction_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding mean and median without zero values\n",
    "df.loc[df['construction_year']!=0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 values in construction_year with mean of data without 0s.\n",
    "df['construction_year'].replace(to_replace = 0, value = 1996, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if it worked\n",
    "df['construction_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude/Longitude zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.longitude.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigate longitude entries that are 0\n",
    "df.loc[df['longitude'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0s that are entered into the longitude column are also 0s in gps_height and -2e8 for latitude columns.  I will drop these values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with 0 entered in longitude column\n",
    "df = df.loc[df['longitude'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check to see if it worked\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it all worked!  I believe the __amount_tsh__ and __population__ 0 values are real so I will leave all data as is for vanilla models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installer - Several different spellings for same installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique values after inital cleaning\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon checking the unique values for our categorical variables after trimming the dataset, installer still has 2024 unique entries, which will be a problem when we create dummies.  We will need to cut down the amount of unique entries to not overload our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate 2024 unique values for installer\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "df['installer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several entries with typos and different variations of the same installer. I will attempt to fix some of the clerical errors and narrow down the amount of unique identifiers we will use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correct variations and misspellings in the installer column\n",
    "df['installer'] = df['installer'].replace(to_replace = ('Central government', 'Tanzania Government',\n",
    "                                          'Cental Government','Tanzania government','Cebtral Government', \n",
    "                                          'Centra Government', 'central government', 'CENTRAL GOVERNMENT', \n",
    "                                          'TANZANIA GOVERNMENT','Central govt', 'Centr', 'Centra govt', \n",
    "                                          'Tanzanian Government', 'Tanzania'), value = 'Central Government')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('District COUNCIL', 'DISTRICT COUNCIL',\n",
    "                                          'Counc','District council','District Counci', \n",
    "                                          'Council', 'COUN', 'Distri', 'District  Council'), \n",
    "                                          value = 'District Council')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('villigers', 'villager', 'villagers', 'Villa', 'Village',\n",
    "                                          'Villi', 'Village Council', 'Village Counil', 'Villages', 'Vill', \n",
    "                                          'Village community', 'Villaers', 'Village Community', 'Villag',\n",
    "                                          'Villege Council', 'Village council', 'Village Council', 'Villagerd', \n",
    "                                          'Villager', 'Village Technician', 'Village Office', 'VILLAGE COUNCIL',\n",
    "                                          'Village community members', 'VILLAG', 'Village Government', \n",
    "                                          'Village govt', 'VILLAGERS', 'Village  Council'), value ='Villagers')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('District Water Department', 'District water depar',\n",
    "                                          'Distric Water Department'), value ='District water department')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('FinW', 'Fini water', 'FINI WATER', 'FIN WATER',\n",
    "                                          'Finwater', 'FINN WATER', 'FinW', 'FW', 'FinWater', 'FiNI WATER'), \n",
    "                                          value ='Fini Water')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('RC CHURCH', 'RC Churc', 'RC', 'RC Ch', 'RC C', 'RC CH',\n",
    "                                          'RC church', 'RC CATHORIC', 'Ch') , value ='RC Church')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('world vision', 'World Division', 'World vision', \n",
    "                                          'WORLD VISION', 'World Vission'), value ='World Vision')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('Unisef','Unicef'), value ='UNICEF')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = 'DANID', value ='DANIDA')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace =('Commu', 'Communit', 'commu', 'COMMU', 'COMMUNITY', \n",
    "                                          'Adra /Community', 'Communit', 'Adra/Community', 'Adra/ Community'), \n",
    "                                          value ='Community')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('GOVERNMENT', 'GOVER', 'GOVERNME', 'GOVERM', 'GOVERN',\n",
    "                                          'Gover', 'Gove', 'Governme', 'Governmen'), value ='Government')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('Hesawa', 'hesawa'), value ='HESAWA')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('JAICA', 'JICA', 'Jica', 'Jeica', 'JAICA CO'), \n",
    "                                          value ='Jaica')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = ('KKKT _ Konde and DWE', 'KKT', 'KKKT Church', 'KkKT'), \n",
    "                                          value ='KKKT')\n",
    "\n",
    "df['installer'] = df['installer'].replace(to_replace = '0', value ='Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['installer'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep installers that have done over 150 waterpoints\n",
    "clean_df = df.loc[(df['installer'] == 'DWE') | (df['installer'] == 'Government') | (df['installer'] == 'Community') \n",
    "                  | (df['installer'] == 'DANIDA') | (df['installer'] == 'RWE') | (df['installer'] == 'District Council') \n",
    "                  | (df['installer'] == 'Central Government') | (df['installer'] == 'KKKT') \n",
    "                  | (df['installer'] == 'Fini Water') | (df['installer'] == 'Unknown') | (df['installer'] == 'TCRS') \n",
    "                  | (df['installer'] == 'World Vision') | (df['installer'] == 'CES') | (df['installer'] == 'RC Church') \n",
    "                  | (df['installer'] == 'Villagers') | (df['installer'] == 'LGA') | (df['installer'] == 'WEDECO') \n",
    "                  | (df['installer'] == 'TASAF') | (df['installer'] == 'Jaica') | (df['installer'] == 'UNICEF') \n",
    "                  | (df['installer'] == 'TWESA') | (df['installer'] == 'AMREF') | (df['installer'] == 'WU') \n",
    "                  | (df['installer'] == 'Dmdd') | (df['installer'] == 'ACRA') | (df['installer'] == 'SEMA') \n",
    "                  | (df['installer'] == 'DW') | (df['installer'] == 'OXFAM') | (df['installer'] == 'Da') \n",
    "                  | (df['installer'] == 'Idara ya maji') | (df['installer'] == 'Sengerema Water Department') \n",
    "                  | (df['installer'] == 'Kiliwater') | (df['installer'] == 'District water department') \n",
    "                  | (df['installer'] == 'NORAD') | (df['installer'] == 'DH') | (df['installer'] == 'DWSP') \n",
    "                  | (df['installer'] == 'Lawatefuka water sup') | (df['installer'] == 'Magadini-Makiwaru wa') \n",
    "                  | (df['installer'] == 'MWE') | (df['installer'] == 'Handeni Trunk Main(') \n",
    "                  | (df['installer'] == 'Is') | (df['installer'] == 'Norad') | (df['installer'] == 'HESAWA')]\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39236 entries, 2 to 59395\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   status_group           39236 non-null  object \n",
      " 1   amount_tsh             39236 non-null  float64\n",
      " 2   gps_height             39236 non-null  int64  \n",
      " 3   installer              39236 non-null  object \n",
      " 4   longitude              39236 non-null  float64\n",
      " 5   latitude               39236 non-null  float64\n",
      " 6   basin                  39236 non-null  object \n",
      " 7   region                 39236 non-null  object \n",
      " 8   population             39236 non-null  int64  \n",
      " 9   permit                 39236 non-null  int64  \n",
      " 10  construction_year      39236 non-null  int64  \n",
      " 11  extraction_type_class  39236 non-null  object \n",
      " 12  management             39236 non-null  object \n",
      " 13  payment_type           39236 non-null  object \n",
      " 14  water_quality          39236 non-null  object \n",
      " 15  quantity               39236 non-null  object \n",
      " 16  source_type            39236 non-null  object \n",
      " 17  waterpoint_type        39236 non-null  object \n",
      "dtypes: float64(3), int64(4), object(11)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Features Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical column boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression analysis was used to build a model to predict housing prices in King County, WA. After 4 iterations from our baseline model, the results of the final regression model (Model D) indicated that 83 predictors explained 85.2% of the variance in the dataset(R2= 0.852). The RMSE of the final model was $108,603.30, which is the error in our price prediction. Difference between train and test set metrics showed that our model was not overfitted or underfitted.  All of the independent variables used in the model were significant predictors of sale price with p-values less than 0.05.\n",
    "\n",
    "Following are features that have the most positive effect on sale price:\n",
    "\n",
    "* For every 1 square foot of living space, the price is increased by 0.03%.\n",
    "* Location is 2nd highest indicator of price. Our top 10 zipcodes (98039, 98004, 98112, 98109, 98119, 98102, 98105, 98040, 98199, 98107) were 142 to 281% higher in price than our baseline zipcode of 98001.\n",
    "\n",
    "Following are features that have the most negative impact on sale price:\n",
    "* Homes with a basement decreases the price by 6.11%.\n",
    "* Homes with 3 floors decreases the price by 7.49%.  More floors is less desirable.\n",
    "\n",
    "Following are some other notable conclusions:\n",
    "\n",
    "* Price for homes with a waterfront are 64.5% higher than homes without a waterfront.\n",
    "* Homes that have been renovated in the last 30 years will increase the price by 8.40%.\n",
    "* Homes with a view rated as 4 (highest rating- i.e. Mt. Rainier, Olympics, Cascades, Territorial, Seattle Skyline, Puget Sound, etc) increase the price by 45.59%.\n",
    "\n",
    "Future work to improve on this model would be to make multiple models in different price ranges (i.e. 100k - 500k, 500k - 1 million, etc). Our model has limitations due to the wide range of prices in the dataset such as an error of $108,000 in predicting the price.  Another interesting extension to this project for future work is adding school districts grade into the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
